{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.contrib import layers, rnn, seq2seq\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fasttext as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_generator import NLUDataGenerator\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "from tensorflow.contrib.legacy_seq2seq import rnn_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataGenDev.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataGen.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(diff):  175\n",
      "tmp len:  175\n",
      "before:  711\n",
      "fin:  886\n"
     ]
    }
   ],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "TimeMajor = False\n",
    "batch_size = 32\n",
    "encoder_max_time = 64\n",
    "\n",
    "DataGenDev = NLUDataGenerator('../data/dev/usr_df_final.csv',\n",
    "                           '../data/ontology_dstc2.json',\n",
    "                           '../data/slots.txt', None, seq_len = encoder_max_time,\n",
    "                           batch_size = batch_size, time_major=TimeMajor)\n",
    "\n",
    "DataGen = NLUDataGenerator('../data/train/usr_df_final.csv',\n",
    "                           '../data/ontology_dstc2.json',\n",
    "                           '../data/slots.txt', DataGenDev.vocab, seq_len = encoder_max_time,\n",
    "                           batch_size = batch_size, time_major=TimeMajor)\n",
    "\n",
    "vocab_size = len(DataGen.vocab)\n",
    "input_embedding_size = 100\n",
    "# encoder_hidden_units = 128\n",
    "# decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'dont', 'care', 'about', 'price']\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['music', 'im', 'looking', 'for', 'a', 'expensive', 'priced', 'restaurant', 'in', 'the', 'centre', 'part', 'of', 'town', 'please']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-pricerange', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['is', 'there', 'on', 'serving', 'chineese']\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['what', 'price', 'ran']\n",
      "['O', 'B-request', 'I-request']\n",
      "['O', 'B-pricerange', 'I-pricerange']\n",
      "-----\n",
      "['id', 'like', 'to', 'find', 'a', 'restaurant', 'serving', 'swiss', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['i', 'dont', 'care', 'about', 'the', 'price', 'range']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['id', 'like', 'a', 'cheap', 'korean', 'restaurant']\n",
      "['O', 'O', 'O', 'B-inform', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'B-pricerange', 'B-food', 'O']\n",
      "-----\n",
      "['shit', 'served', 'persian', 'food']\n",
      "['O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['no', 'american', 'food']\n",
      "['B-negate', 'B-negate', 'B-negate']\n",
      "['B-negate', 'B-negate', 'B-negate']\n",
      "-----\n",
      "['is', 'that', 'greek']\n",
      "['O', 'O', 'B-confirm']\n",
      "['O', 'O', 'B-food']\n",
      "-----\n",
      "['i', 'would', 'like', 'a', 'cheap', 'restaurant', 'in', 'the', 'west', 'part', 'of', 'town']\n",
      "['O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'B-pricerange', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O']\n",
      "-----\n",
      "['dontcare', 'price', 'range', 'is', 'fine']\n",
      "['B-inform', 'O', 'O', 'O', 'O']\n",
      "['B-pricerange', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['any', 'of', 'town']\n",
      "['O', 'O', 'O']\n",
      "['O', 'O', 'O']\n",
      "-----\n",
      "['i', 'want', 'to', 'find', 'a', 'restaurant', 'in', 'the', 'south', 'part', 'of', 'town', 'and', 'it', 'should', 'serve']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['thank', 'you', 'good', 'bye']\n",
      "['B-bye', 'B-bye', 'B-bye', 'B-bye']\n",
      "['B-bye', 'B-bye', 'B-bye', 'B-bye']\n",
      "-----\n",
      "['want', 'something', 'in', 'the', 'dontcare', 'side', 'of', 'town', 'thats', 'expensive', 'priced']\n",
      "['O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'B-pricerange', 'O']\n",
      "-----\n",
      "['im', 'looking', 'for', 'a', 'restaurant', 'in', 'central', 'it', 'should', 'serve', 'cuban', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['cool', 'whats', 'the', 'telephone']\n",
      "['O', 'O', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'B-phone']\n",
      "-----\n",
      "['what', 'is', 'the', 'address']\n",
      "['O', 'O', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'B-addr']\n",
      "-----\n",
      "['and', 'its', 'addrss']\n",
      "['O', 'O', 'B-request']\n",
      "['O', 'O', 'B-addr']\n",
      "-----\n",
      "['i', 'am', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'east', 'part', 'of', 'town', 'and', 'it', 'should', 'serve', 'lebanese', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['the', 'addre', 'number', 'and', 'postcode']\n",
      "['O', 'B-request', 'B-request', 'O', 'B-request']\n",
      "['O', 'B-addr', 'B-phone', 'O', 'B-postcode']\n",
      "-----\n",
      "['id', 'like', 'a', 'restaurant', 'in', 'the', 'north', 'part', 'of', 'town', 'that', 'serves']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['whats', 'his', 'address']\n",
      "['O', 'O', 'B-request']\n",
      "['O', 'O', 'B-addr']\n",
      "-----\n",
      "['im', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'centre', 'part', 'of', 'town', 'and', 'mexican', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['im', 'looking', 'for', 'panasian', 'food', 'in', 'the', 'moderate', 'price', 'range']\n",
      "['O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'B-inform', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-food', 'O', 'O', 'O', 'B-pricerange', 'O', 'O']\n",
      "-----\n",
      "['belgium', 'food']\n",
      "['O', 'O']\n",
      "['O', 'O']\n",
      "-----\n",
      "['food', 'what']\n",
      "['B-request', 'O']\n",
      "['B-food', 'O']\n",
      "-----\n",
      "['i', 'want', 'to', 'know', 'the', 'addrss', 'and', 'the']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-request', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-addr', 'O', 'O']\n",
      "-----\n",
      "['looking', 'for', 'a', 'dontcare', 'priced', 'restaurant']\n",
      "['O', 'O', 'O', 'B-inform', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-pricerange', 'O', 'O']\n",
      "-----\n",
      "['i', 'need', 'one', 'in', 'the', 'moderate', 'price', 'range']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-pricerange', 'O', 'O']\n",
      "-----\n",
      "['alright', 'whats', 'the', 'addrss']\n",
      "['O', 'O', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'B-addr']\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "x, m1, m2, _lens = next(DataGen)\n",
    "for x_, m1_, m2_ in zip(x, m1, m2):\n",
    "    print(DataGen.decode_sentence(x_))\n",
    "    print(DataGen.decode_acts(m2_))\n",
    "    print(DataGen.decode_slots(m1_))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slots_number:  26\n",
      "acts_number:  18\n",
      "vocab_size:  886\n"
     ]
    }
   ],
   "source": [
    "slots_number = len(DataGen.slots_encode)\n",
    "acts_number = len(DataGen.acts_encode)\n",
    "print(\"slots_number: \", slots_number)\n",
    "print(\"acts_number: \", acts_number)\n",
    "print(\"vocab_size: \", DataGen.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING EMBEDDING MATRIX\n",
    "\n",
    "fs_model = fs.load_model(\"./model.bin\")\n",
    "embedd = np.zeros((vocab_size, input_embedding_size), dtype=np.float32)\n",
    "for k, v in DataGen.vocab.items():\n",
    "    embedd[v] = fs_model[k]\n",
    "embedd[0] = [0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "mask1 = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "mask2 = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "lens = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "embeddings = tf.get_variable('embeddings', initializer=embedd, dtype=tf.float32, trainable=False)\n",
    "\n",
    "encoded = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "cell = LSTMCell(input_embedding_size)\n",
    "\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell, encoded, sequence_length=lens, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_mask = tf.to_float(tf.greater(inputs, PAD))\n",
    "with tf.variable_scope('lm-reg'):\n",
    "    first_words = tf.slice(inputs, [0, 1], [-1, -1])\n",
    "    target = tf.pad(first_words, [[0, 0], [0, 1]])\n",
    "    nwp = layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    lm_loss = seq2seq.sequence_loss(nwp, target, seq_mask)\n",
    "\n",
    "with tf.variable_scope('mask-pred'):\n",
    "    m1p = layers.fully_connected(outputs, slots_number, activation_fn=None)\n",
    "    m2p = layers.fully_connected(outputs, acts_number, activation_fn=None)\n",
    "    \n",
    "    prediction_m1 = tf.argmax(tf.nn.softmax(m1p), axis = 2)\n",
    "    prediction_m2 = tf.argmax(tf.nn.softmax(m2p), axis = 2)\n",
    "    \n",
    "    m1_loss = seq2seq.sequence_loss(m1p, mask1, seq_mask)\n",
    "    m2_loss = seq2seq.sequence_loss(m2p, mask2, seq_mask)\n",
    "    \n",
    "    \n",
    "lr = tf.Variable(0.01, trainable=False)\n",
    "opt = tf.train.GradientDescentOptimizer(lr)\n",
    "train_op = opt.minimize(lm_loss + m1_loss + m2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "6.79 3.37 2.85:   0%|          | 1/1000 [00:00<02:03,  8.07it/s]\u001b[A\n",
      "6.79 3.31 2.79:   1%|          | 6/1000 [00:00<01:32, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG_loss:  [ 6.7941494   3.37402844  2.85030293]\n",
      "step:  0\n",
      "INPUT:  ['adddress', 'telephone']\n",
      "m2_tar:  ['B-inform', 'B-inform']\n",
      "m2_pred:  ['I-bye', 'I-bye', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['B-addr', 'B-phone']\n",
      "m1_pred:  ['B-name', 'I-addr', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['and', 'telephone', 'nubmer']\n",
      "m2_tar:  ['O', 'B-inform', 'O']\n",
      "m2_pred:  ['I-bye', 'I-bye', 'I-bye', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'B-phone', 'O']\n",
      "m1_pred:  ['I-food', 'B-name', 'I-addr', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['doesnt', 'matter']\n",
      "m2_tar:  ['O', 'O']\n",
      "m2_pred:  ['O', 'B-hello', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'O']\n",
      "m1_pred:  ['I-hello', 'I-hello', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "6.79 3.20 2.67:   1%|          | 11/1000 [00:00<01:10, 14.06it/s]\u001b[A\n",
      "6.78 3.05 2.52:   2%|▏         | 16/1000 [00:00<00:55, 17.86it/s]\u001b[A\n",
      "6.77 2.89 2.35:   2%|▏         | 21/1000 [00:00<00:44, 21.98it/s]\u001b[A\n",
      "6.76 2.72 2.19:   3%|▎         | 26/1000 [00:00<00:37, 26.16it/s]\u001b[A\n",
      "6.76 2.56 2.04:   3%|▎         | 31/1000 [00:00<00:32, 30.07it/s]\u001b[A\n",
      "6.75 2.39 1.90:   4%|▎         | 36/1000 [00:00<00:28, 33.72it/s]\u001b[A\n",
      "6.73 2.24 1.78:   4%|▍         | 42/1000 [00:00<00:25, 37.02it/s]\u001b[A\n",
      "6.73 2.16 1.72:   5%|▍         | 47/1000 [00:01<00:23, 40.13it/s]\u001b[A\n",
      "6.71 2.02 1.61:   5%|▌         | 53/1000 [00:01<00:21, 43.11it/s]\u001b[A\n",
      "6.70 1.91 1.52:   6%|▌         | 58/1000 [00:01<00:21, 44.37it/s]\u001b[A\n",
      "6.68 1.81 1.45:   6%|▋         | 63/1000 [00:01<00:20, 45.39it/s]\u001b[A\n",
      "6.67 1.78 1.44:   7%|▋         | 69/1000 [00:01<00:19, 47.39it/s]\u001b[A\n",
      "6.65 1.73 1.41:   7%|▋         | 74/1000 [00:01<00:19, 46.75it/s]\u001b[A\n",
      "6.63 1.65 1.35:   8%|▊         | 80/1000 [00:01<00:19, 48.19it/s]\u001b[A\n",
      "6.61 1.60 1.31:   9%|▊         | 86/1000 [00:01<00:18, 49.11it/s]\u001b[A\n",
      "6.59 1.56 1.28:   9%|▉         | 92/1000 [00:01<00:17, 50.62it/s]\u001b[A\n",
      "6.58 1.56 1.29:  10%|▉         | 98/1000 [00:02<00:17, 50.34it/s]\u001b[A\n",
      "6.55 1.53 1.26:  10%|█         | 104/1000 [00:02<00:18, 47.49it/s]\u001b[A\n",
      "6.54 1.49 1.23:  11%|█         | 109/1000 [00:02<00:18, 47.55it/s]\u001b[A\n",
      "6.52 1.48 1.21:  11%|█▏        | 114/1000 [00:02<00:18, 47.02it/s]\u001b[A\n",
      "6.50 1.53 1.27:  12%|█▏        | 119/1000 [00:02<00:18, 47.28it/s]\u001b[A\n",
      "6.48 1.50 1.23:  12%|█▏        | 124/1000 [00:02<00:19, 45.09it/s]\u001b[A\n",
      "6.46 1.52 1.25:  13%|█▎        | 129/1000 [00:02<00:19, 45.68it/s]\u001b[A\n",
      "6.45 1.52 1.26:  13%|█▎        | 134/1000 [00:02<00:19, 45.45it/s]\u001b[A\n",
      "6.43 1.50 1.24:  14%|█▍        | 139/1000 [00:03<00:18, 46.17it/s]\u001b[A\n",
      "6.41 1.47 1.22:  14%|█▍        | 144/1000 [00:03<00:18, 45.61it/s]\u001b[A\n",
      "6.39 1.44 1.20:  15%|█▌        | 150/1000 [00:03<00:18, 47.04it/s]\u001b[A\n",
      "6.36 1.44 1.18:  16%|█▌        | 156/1000 [00:03<00:17, 48.60it/s]\u001b[A\n",
      "6.34 1.47 1.21:  16%|█▌        | 161/1000 [00:03<00:17, 48.67it/s]\u001b[A\n",
      "6.32 1.41 1.15:  17%|█▋        | 166/1000 [00:03<00:17, 46.92it/s]\u001b[A\n",
      "6.30 1.40 1.14:  17%|█▋        | 171/1000 [00:03<00:17, 46.80it/s]\u001b[A\n",
      "6.28 1.43 1.17:  18%|█▊        | 177/1000 [00:03<00:16, 48.54it/s]\u001b[A\n",
      "6.25 1.39 1.13:  18%|█▊        | 183/1000 [00:03<00:16, 49.72it/s]\u001b[A\n",
      "6.23 1.42 1.16:  19%|█▉        | 189/1000 [00:04<00:15, 51.34it/s]\u001b[A\n",
      "6.21 1.41 1.15:  20%|█▉        | 195/1000 [00:04<00:16, 49.21it/s]\u001b[A\n",
      "6.18 1.42 1.16:  20%|██        | 201/1000 [00:04<00:16, 49.52it/s]\u001b[A\n",
      "6.16 1.43 1.17:  21%|██        | 207/1000 [00:04<00:16, 49.51it/s]\u001b[A\n",
      "6.14 1.45 1.17:  21%|██        | 212/1000 [00:04<00:16, 46.51it/s]\u001b[A\n",
      "6.12 1.37 1.11:  22%|██▏       | 217/1000 [00:04<00:16, 46.07it/s]\u001b[A\n",
      "6.09 1.29 1.03:  22%|██▏       | 222/1000 [00:04<00:16, 46.53it/s]\u001b[A\n",
      "6.07 1.33 1.09:  23%|██▎       | 228/1000 [00:04<00:16, 47.79it/s]\u001b[A\n",
      "6.04 1.31 1.07:  23%|██▎       | 234/1000 [00:04<00:15, 49.35it/s]\u001b[A\n",
      "6.03 1.34 1.09:  24%|██▍       | 239/1000 [00:05<00:16, 47.00it/s]\u001b[A\n",
      "6.00 1.33 1.07:  24%|██▍       | 245/1000 [00:05<00:15, 48.12it/s]\u001b[A\n",
      "5.98 1.36 1.10:  25%|██▌       | 250/1000 [00:05<00:15, 48.56it/s]\u001b[A\n",
      "5.96 1.38 1.14:  26%|██▌       | 255/1000 [00:05<00:15, 47.79it/s]\u001b[A\n",
      "5.94 1.36 1.11:  26%|██▌       | 261/1000 [00:05<00:15, 48.76it/s]\u001b[A\n",
      "5.92 1.36 1.12:  27%|██▋       | 266/1000 [00:05<00:15, 47.83it/s]\u001b[A\n",
      "5.91 1.41 1.16:  27%|██▋       | 271/1000 [00:05<00:15, 48.22it/s]\u001b[A\n",
      "5.52 1.24 1.01:  41%|████      | 406/1000 [00:08<00:12, 48.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG_loss:  [ 5.54633522  1.29391539  1.05486703]\n",
      "step:  400\n",
      "INPUT:  ['chinese', 'food', 'in', 'dontcare', 'area']\n",
      "m2_tar:  ['B-request', 'O', 'O', 'B-request', 'O']\n",
      "m2_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['B-food', 'O', 'O', 'B-area', 'O']\n",
      "m1_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['thank', 'you', 'goodbye']\n",
      "m2_tar:  ['B-bye', 'B-bye', 'B-bye']\n",
      "m2_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['B-bye', 'B-bye', 'B-bye']\n",
      "m1_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['what', 'cost', 'is', 'this', 'venue']\n",
      "m2_tar:  ['O', 'B-inform', 'O', 'O', 'O']\n",
      "m2_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'B-pricerange', 'O', 'O', 'O']\n",
      "m1_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.87 1.09 0.89:  81%|████████  | 806/1000 [00:16<00:03, 49.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG_loss:  [ 4.90570879  1.09427881  0.8916809 ]\n",
      "step:  800\n",
      "INPUT:  ['is', 'there', 'a', 'cheap', 'priced', 'restaurant', 'that', 'serves', 'caribbean', 'food']\n",
      "m2_tar:  ['O', 'O', 'O', 'B-affirm', 'O', 'O', 'O', 'O', 'B-affirm', 'O']\n",
      "m2_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'O', 'O', 'B-pricerange', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "m1_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['how', 'about', 'a', 'expensive', 'priced']\n",
      "m2_tar:  ['O', 'O', 'O', 'B-request', 'O']\n",
      "m2_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'O', 'O', 'B-pricerange', 'O']\n",
      "m1_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['a', 'restaurant', 'that', 'serves', 'african', 'food']\n",
      "m2_tar:  ['O', 'O', 'O', 'O', 'B-request', 'O']\n",
      "m2_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "m1_pred:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.83 1.01 0.81: 100%|██████████| 1000/1000 [00:20<00:00, 47.88it/s]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    avg_loss = -np.ones((3, ))\n",
    "    alpha = 0.9\n",
    "    \n",
    "    pbar = tqdm(range(1000))\n",
    "    for step in pbar:\n",
    "        x, m1, m2, _lens = next(DataGen)\n",
    "        \n",
    "        res = sess.run([lm_loss, m1_loss, m2_loss, train_op], {inputs: x, mask1: m1, mask2: m2, lens: _lens})\n",
    "        res = np.array(res[:-1])\n",
    "        \n",
    "        if avg_loss[0] < 0:\n",
    "            avg_loss = res\n",
    "        \n",
    "        avg_loss = alpha * avg_loss + (1-alpha) * res\n",
    "        \n",
    "        pbar.set_description(' '.join('{:.2f}'.format(_) for _ in avg_loss))\n",
    "        \n",
    "        if(step%400 == 0):\n",
    "            \n",
    "            x_dev, m1_dev, m2_dev, _lens_dev = next(DataGenDev)\n",
    "            print(\"AVG_loss: \", avg_loss)\n",
    "            print(\"step: \", step)\n",
    "            \n",
    "            N = 3 # how many examples show to test:\n",
    "            pred_1, pred_2 = sess.run([prediction_m1, prediction_m2], {inputs: x_dev[:N], \n",
    "                                                              mask1: m1_dev[:N], \n",
    "                                                              mask2: m2_dev[:N], \n",
    "                                                              lens: _lens_dev[:N]})\n",
    "            \n",
    "            for x_, m1_, m2_, p1, p2 in zip(x_dev[:N], m1_dev[:N], m2_dev[:N], pred_1, pred_2):\n",
    "\n",
    "                print(\"INPUT: \", DataGen.decode_sentence(x_))\n",
    "                print(\"m2_tar: \", DataGen.decode_acts(m2_))\n",
    "                print(\"m2_pred: \", DataGen.decode_acts(p2))                \n",
    "                \n",
    "                print(\"-----\")\n",
    "                \n",
    "                print(\"m1_tar: \", DataGen.decode_slots(m1_))\n",
    "                print(\"m1_pred: \", DataGen.decode_slots(p1))\n",
    "                \n",
    "                print(\"==========================\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('../logdir/', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Это чисто черновик, где происходит какая-то фигня с bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "encoder_targets = tf.placeholder(shape=(None), dtype=tf.int32, name='encoder_targets')\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell_b = LSTMCell(encoder_hidden_units)\n",
    "encoder_cell_f = LSTMCell(encoder_hidden_units, reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder_cell = LSTMCell(encoder_hidden_units, reuse=tf.get_variable_scope().reuse)\n",
    "# encoder_cell_f = LSTMCell(encoder_hidden_units)\n",
    "((encoder_fw_outputs,\n",
    "encoder_bw_outputs),\n",
    "(encoder_fw_final_state,\n",
    "encoder_bw_final_state)) = (tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell_f,\n",
    "                                cell_bw=encoder_cell_b,\n",
    "                                inputs=encoder_inputs_embedded,\n",
    "                                sequence_length=encoder_inputs_length,\n",
    "                                dtype=tf.float32, time_major=TimeMajor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers\n",
    "next_word_pred = layers.fully_connected(encoder_outputs, vocab_size, ...)\n",
    "\n",
    "\n",
    "\n",
    "mask1_pred = layers.fully_connected(encoder_outputs, mask1_vocab, ...)\n",
    "mask2_pred = layers.fully_connected(encoder_outputs, mask2_vocab, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_final_state_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([encoder_hidden_units*2, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.add(tf.matmul(encoder_final_state_h, W), b)\n",
    "# encoder_logits = tf.reshape(encoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = encoder_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
