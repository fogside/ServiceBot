{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.contrib import layers, rnn, seq2seq\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fasttext as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_generator import NLUDataGenerator\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "from tensorflow.contrib.legacy_seq2seq import rnn_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "fastText: Cannot load ../GoogleNews-vectors-negative300.bin due to C++ extension failed to allocate the memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.load_model (fasttext/fasttext.cpp:4234)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vector::_M_default_append",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ab944fe6d408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_fs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.load_model (fasttext/fasttext.cpp:4292)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: fastText: Cannot load ../GoogleNews-vectors-negative300.bin due to C++ extension failed to allocate the memory"
     ]
    }
   ],
   "source": [
    "model_fs = fs.load_model(\"../GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataGenDev.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataGen.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "TimeMajor = False\n",
    "batch_size = 32\n",
    "encoder_max_time = 64\n",
    "DataGen = NLUDataGenerator('../data/train/usr_df_final.csv',\n",
    "                           '../data/ontology_dstc2.json',\n",
    "                           '../data/slots.txt', seq_len = encoder_max_time,\n",
    "                           batch_size = batch_size, time_major=TimeMajor)\n",
    "\n",
    "DataGenDev = NLUDataGenerator('../data/dev/usr_df_final.csv',\n",
    "                           '../data/ontology_dstc2.json',\n",
    "                           '../data/slots.txt', seq_len = encoder_max_time,\n",
    "                           batch_size = batch_size, time_major=TimeMajor)\n",
    "\n",
    "vocab_size = len(DataGen.vocab)\n",
    "input_embedding_size = 128\n",
    "encoder_hidden_units = 128\n",
    "# decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dontcare', 'restaurant', 'serves']\n",
      "['B-inform', 'O', 'O']\n",
      "['B-pricerange', 'O', 'O']\n",
      "-----\n",
      "['i', 'want', 'to', 'find', 'a', 'expensive', 'priced', 'restaurant', 'that', 'serves']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-pricerange', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['okay', 'thats', 'all', 'thank', 'you', 'good', 'bye']\n",
      "['B-bye', 'B-bye', 'B-bye', 'B-bye', 'B-bye', 'B-bye', 'B-bye']\n",
      "['B-bye', 'B-bye', 'B-bye', 'B-bye', 'B-bye', 'B-bye', 'B-bye']\n",
      "-----\n",
      "['hi', 'im', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'centre', 'part', 'of', 'town', 'that', 'serves', 'baskaye', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['hi', 'im', 'looking', 'for', 'a', 'rerestaurant', 'that', 'serves', 'singaporean', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['id', 'like', 'a', 'restaurant', 'that', 'serves', 'indonesian', 'asian', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-food', 'O', 'O']\n",
      "-----\n",
      "['id', 'like', 'a', 'restaurant', 'in', 'the', 'east', 'part', 'of', 'town']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O']\n",
      "-----\n",
      "['address', 'and', 'type', 'of', 'serves']\n",
      "['B-request', 'O', 'O', 'O', 'B-request']\n",
      "['B-addr', 'O', 'O', 'O', 'B-food']\n",
      "-----\n",
      "['i', 'need', 'to', 'find', 'a', 'moderate', 'priced', 'restaurant']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-pricerange', 'O', 'O']\n",
      "-----\n",
      "['find', 'restaurant', 'in', 'dontcare', 'part', 'of', 'town', 'serving', 'asian', 'food']\n",
      "['O', 'O', 'O', 'B-inform', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-area', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['can', 'i', 'get', 'the', 'addrss', 'and', 'part', 'of', 'town', 'of', 'the', 'venue']\n",
      "['O', 'O', 'O', 'O', 'B-request', 'O', 'B-request', 'I-request', 'I-request', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'B-addr', 'O', 'B-area', 'I-area', 'I-area', 'O', 'O', 'O']\n",
      "-----\n",
      "['excellent', 'can', 'you', 'give', 'me', 'the', 'telephone', 'please']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-request', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-phone', 'O']\n",
      "-----\n",
      "['i', 'need', 'to', 'find', 'a', 'restaurant', 'in', 'the', 'center', 'and', 'it', 'should', 'serve', 'cuban', 'food']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-food', 'O']\n",
      "-----\n",
      "['im', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'center', 'of', 'town', 'that', 'serves']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "['how', 'about', 'the', 'dontcare', 'part', 'of', 'town']\n",
      "['O', 'O', 'O', 'B-inform', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-area', 'O', 'O', 'O']\n",
      "-----\n",
      "['find', 'australian']\n",
      "['O', 'B-inform']\n",
      "['O', 'B-food']\n",
      "-----\n",
      "['any', 'kind']\n",
      "['O', 'O']\n",
      "['O', 'O']\n",
      "-----\n",
      "['ah', 'west']\n",
      "['O', 'B-inform']\n",
      "['O', 'B-area']\n",
      "-----\n",
      "['addre', 'number', 'and', 'cost']\n",
      "['B-request', 'B-request', 'O', 'B-request']\n",
      "['B-addr', 'B-phone', 'O', 'B-pricerange']\n",
      "-----\n",
      "['i', 'am', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'west', 'part', 'of', 'town']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O']\n",
      "-----\n",
      "['how', 'about', 'swedish']\n",
      "['O', 'O', 'B-inform']\n",
      "['O', 'O', 'B-food']\n",
      "-----\n",
      "['can', 'you', 'give', 'me', 'the', 'adddress', 'and', 'phone']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-request', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-addr', 'O', 'B-phone']\n",
      "-----\n",
      "['and', 'the', 'part', 'of', 'town']\n",
      "['O', 'O', 'B-request', 'I-request', 'I-request']\n",
      "['O', 'O', 'B-area', 'I-area', 'I-area']\n",
      "-----\n",
      "['center', 'town']\n",
      "['O', 'O']\n",
      "['O', 'O']\n",
      "-----\n",
      "['north', 'east']\n",
      "['O', 'B-inform']\n",
      "['O', 'B-area']\n",
      "-----\n",
      "['what', 'about', 'in', 'the', 'west', 'side', 'of', 'town']\n",
      "['O', 'O', 'O', 'O', 'B-inform', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'B-area', 'O', 'O', 'O']\n",
      "-----\n",
      "['whats', 'its', 'cost']\n",
      "['O', 'O', 'B-request']\n",
      "['O', 'O', 'B-pricerange']\n",
      "-----\n",
      "['and', 'whats', 'the', 'address']\n",
      "['O', 'O', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'B-addr']\n",
      "-----\n",
      "['okay', 'and', 'uh', 'the', 'phone']\n",
      "['O', 'O', 'O', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'O', 'B-phone']\n",
      "-----\n",
      "['what', 'about', 'gasper', 'hungarian', 'type', 'o']\n",
      "['O', 'O', 'O', 'B-inform', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-food', 'O', 'O']\n",
      "-----\n",
      "['may', 'i', 'have', 'the', 'phone']\n",
      "['O', 'O', 'O', 'O', 'B-request']\n",
      "['O', 'O', 'O', 'O', 'B-phone']\n",
      "-----\n",
      "['can', 'i', 'get', 'the', 'addre', 'and', 'telephone', 'please']\n",
      "['O', 'O', 'O', 'O', 'B-request', 'O', 'B-request', 'O']\n",
      "['O', 'O', 'O', 'O', 'B-addr', 'O', 'B-phone', 'O']\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "x, m1, m2, _lens = next(DataGen)\n",
    "for x_, m1_, m2_ in zip(x, m1, m2):\n",
    "    print(DataGen.decode_sentence(x_))\n",
    "    print(DataGen.decode_acts(m2_))\n",
    "    print(DataGen.decode_slots(m1_))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slots_number:  27\n",
      "acts_number:  19\n",
      "vocab_size:  822\n"
     ]
    }
   ],
   "source": [
    "slots_number = len(DataGen.slots_encode)\n",
    "acts_number = len(DataGen.acts_encode)\n",
    "print(\"slots_number: \", slots_number)\n",
    "print(\"acts_number: \", acts_number)\n",
    "print(\"vocab_size: \", DataGen.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "mask1 = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "mask2 = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "lens = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "embeddings = tf.get_variable('embeddings', [vocab_size, input_embedding_size], tf.float32)\n",
    "\n",
    "encoded = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "cell = LSTMCell(input_embedding_size)\n",
    "\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell, encoded, sequence_length=lens, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_mask = tf.to_float(tf.greater(inputs, PAD))\n",
    "with tf.variable_scope('lm-reg'):\n",
    "    first_words = tf.slice(inputs, [0, 1], [-1, -1])\n",
    "    target = tf.pad(first_words, [[0, 0], [0, 1]])\n",
    "    nwp = layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    lm_loss = seq2seq.sequence_loss(nwp, target, seq_mask)\n",
    "\n",
    "with tf.variable_scope('mask-pred'):\n",
    "    m1p = layers.fully_connected(outputs, slots_number, activation_fn=None)\n",
    "    m2p = layers.fully_connected(outputs, acts_number, activation_fn=None)\n",
    "    \n",
    "    prediction_m1 = tf.argmax(m1p, axis = 2)\n",
    "    prediction_m2 = tf.argmax(m2p, axis = 2)\n",
    "    \n",
    "    m1_loss = seq2seq.sequence_loss(m1p, mask1, seq_mask)\n",
    "    m2_loss = seq2seq.sequence_loss(m2p, mask2, seq_mask)\n",
    "    \n",
    "    \n",
    "lr = tf.Variable(0.01, trainable=False)\n",
    "opt = tf.train.GradientDescentOptimizer(lr)\n",
    "train_op = opt.minimize(lm_loss + m1_loss + m2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG_loss:  [ 6.71100521  3.29597974  2.94340706]\n",
      "step:  0\n",
      "INPUT:  ['how', 'about', 'the', 'address']\n",
      "m2_tar:  ['O', 'O', 'O', 'B-request']\n",
      "m2_pred:  ['I-reqalts', 'I-hello', 'B-bye', 'I-inform', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['O', 'O', 'O', 'B-addr']\n",
      "m1_pred:  ['B-negate', 'I-affirm', 'I-affirm', 'I-affirm', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "==========================\n",
      "INPUT:  ['north', 'part', 'of', 'town', 'german', 'food']\n",
      "m2_tar:  ['B-inform', 'O', 'O', 'O', 'B-inform', 'O']\n",
      "m2_pred:  ['O', 'I-confirm', 'I-bye', 'I-bye', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----\n",
      "m1_tar:  ['B-area', 'O', 'O', 'O', 'B-food', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bcc8ad21e838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m1_tar: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataGenDev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m1_pred: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataGenDev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_acts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==========================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fogside/ServiceBot/notebooks/data_generator.py\u001b[0m in \u001b[0;36mdecode_acts\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'PAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fogside/ServiceBot/notebooks/data_generator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'PAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    avg_loss = -np.ones((3, ))\n",
    "    alpha = 0.9\n",
    "    \n",
    "    pbar = tqdm(range(500))\n",
    "    for step in pbar:\n",
    "        x, m1, m2, _lens = next(DataGen)\n",
    "        \n",
    "        res = sess.run([lm_loss, m1_loss, m2_loss, train_op], {inputs: x, mask1: m1, mask2: m2, lens: _lens})\n",
    "        res = np.array(res[:-1])\n",
    "        \n",
    "        if avg_loss[0] < 0:\n",
    "            avg_loss = res\n",
    "        \n",
    "        avg_loss = alpha * avg_loss + (1-alpha) * res\n",
    "        \n",
    "        pbar.set_description(' '.join('{:.2f}'.format(_) for _ in avg_loss))\n",
    "        \n",
    "        if(step%100 == 0):\n",
    "            \n",
    "            x_dev, m1_dev, m2_dev, _lens_dev = next(DataGenDev)\n",
    "            print(\"AVG_loss: \", avg_loss)\n",
    "            print(\"step: \", step)\n",
    "            \n",
    "            N = 5 # how many examples show to test:\n",
    "            pred_1, pred_2 = sess.run([prediction_m1, prediction_m2], {inputs: x_dev[:N], \n",
    "                                                              mask1: m1_dev[:N], \n",
    "                                                              mask2: m2_dev[:N], \n",
    "                                                              lens: _lens_dev[:N]})\n",
    "            \n",
    "            for x_, m1_, m2_, p1, p2 in zip(x_dev[:N], m1_dev[:N], m2_dev[:N], pred_1, pred_2):\n",
    "                print(\"INPUT: \", DataGenDev.decode_sentence(x_))\n",
    "                print(\"m2_tar: \", DataGenDev.decode_acts(m2_))\n",
    "                print(\"m2_pred: \", DataGenDev.decode_acts(p2))                \n",
    "                \n",
    "                print(\"-----\")\n",
    "                \n",
    "                print(\"m1_tar: \", DataGenDev.decode_slots(m1_))\n",
    "                print(\"m1_pred: \", DataGenDev.decode_acts(p1))\n",
    "                \n",
    "                print(\"==========================\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('../logdir/', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Это чисто черновик, где происходит какая-то фигня с bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "encoder_targets = tf.placeholder(shape=(None), dtype=tf.int32, name='encoder_targets')\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell_b = LSTMCell(encoder_hidden_units)\n",
    "encoder_cell_f = LSTMCell(encoder_hidden_units, reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder_cell = LSTMCell(encoder_hidden_units, reuse=tf.get_variable_scope().reuse)\n",
    "# encoder_cell_f = LSTMCell(encoder_hidden_units)\n",
    "((encoder_fw_outputs,\n",
    "encoder_bw_outputs),\n",
    "(encoder_fw_final_state,\n",
    "encoder_bw_final_state)) = (tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell_f,\n",
    "                                cell_bw=encoder_cell_b,\n",
    "                                inputs=encoder_inputs_embedded,\n",
    "                                sequence_length=encoder_inputs_length,\n",
    "                                dtype=tf.float32, time_major=TimeMajor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers\n",
    "next_word_pred = layers.fully_connected(encoder_outputs, vocab_size, ...)\n",
    "\n",
    "\n",
    "\n",
    "mask1_pred = layers.fully_connected(encoder_outputs, mask1_vocab, ...)\n",
    "mask2_pred = layers.fully_connected(encoder_outputs, mask2_vocab, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_final_state_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([encoder_hidden_units*2, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.add(tf.matmul(encoder_final_state_h, W), b)\n",
    "# encoder_logits = tf.reshape(encoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = encoder_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
